{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iALDsPwM7JId"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. One-Hot Encoding"
      ],
      "metadata": {
        "id": "mhy5p4Jp7Vkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "word_dict = {\n",
        "    'school' : np.array([[1, 0, 0]]),\n",
        "    'study'  : np.array([[0, 1, 0]]),\n",
        "    'workout': np.array([[0, 0, 1]])\n",
        "}\n",
        "\n",
        "cosine_school_study   = cosine_similarity(word_dict['school'], word_dict['study'])\n",
        "cosine_school_workout = cosine_similarity(word_dict['school'], word_dict['workout'])\n",
        "\n",
        "print(f'cosine-similarity between schooland study  : {cosine_school_study}')\n",
        "print(f'cosine-similarity between schooland workout: {cosine_school_workout}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rZvtUYP7Xt4",
        "outputId": "a57fc68e-47ee-4f9b-bde5-6109cd3b9073"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine-similarity between schooland study  : [[0.]]\n",
            "cosine-similarity between schooland workout: [[0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bag of Words"
      ],
      "metadata": {
        "id": "xR3oUVhSB8DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I love AI and AI loves me\",\n",
        "    \"AI is amazing and I love it\",\n",
        "    \"AI is changing the world\",\n",
        "    \"I love learning AI\",\n",
        "    \"AI makes life easier\"\n",
        "]\n",
        "\n",
        "# 단어 사전 만들기\n",
        "word_set = set()\n",
        "for doc in documents:\n",
        "    words = doc.lower().split()  # 소문자로 변환 후 단어 분리\n",
        "    word_set.update(words)\n",
        "\n",
        "# 정렬하여 인덱싱 고정\n",
        "word_list = sorted(word_set)\n",
        "print(\"✅ 단어 리스트:\", word_list)\n",
        "\n",
        "# 문장별 단어 등장 횟수 계산(BoW 행렬 생성)\n",
        "bow_matrix = []\n",
        "for doc in documents:\n",
        "    words = doc.lower().split()\n",
        "    word_count = [words.count(word) for word in word_list]\n",
        "    bow_matrix.append(word_count)\n",
        "\n",
        "print(\"✅ BoW 행렬:\")\n",
        "for row in bow_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJeIEotbHhMt",
        "outputId": "d5f2296a-f393-496c-9eb1-86490be39459"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 단어 리스트: ['ai', 'amazing', 'and', 'changing', 'easier', 'i', 'is', 'it', 'learning', 'life', 'love', 'loves', 'makes', 'me', 'the', 'world']\n",
            "✅ BoW 행렬:\n",
            "[2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
            "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "[1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Word2Vec: CBOW"
      ],
      "metadata": {
        "id": "0-jnzTEXHhQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 1. 샘플 문장 데이터 (단순한 문장 몇 개)\n",
        "corpus = [\n",
        "  \"AI is changing the world\",\n",
        "  \"I love learning AI\",\n",
        "  \"AI is powerful and useful\",\n",
        "  \"Deep learning is a part of AI\",\n",
        "  \"Natural language processing is exciting\"\n",
        "]\n",
        "\n",
        "# 2. 데이터 전처리: 단어 토큰화\n",
        "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
        "\n",
        "# 3. 단어 사전 구축\n",
        "word_list = sorted(set(sum(tokenized_corpus, [])))  # 중복 없는 단어 리스트\n",
        "word2idx = {word: i for i, word in enumerate(word_list)}  # 단어 -> 인덱스\n",
        "idx2word = {i: word for word, i in word2idx.items()}  # 인덱스 -> 단어\n",
        "vocab_size = len(word_list)  # 단어 개수\n",
        "\n",
        "print(\"\\n✅ 단어 사전:\", word2idx)"
      ],
      "metadata": {
        "id": "_r9DMIq9HhTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7457ff-3cf0-4f7d-8e4f-8c072587cfcb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 단어 사전: {'a': 0, 'ai': 1, 'and': 2, 'changing': 3, 'deep': 4, 'exciting': 5, 'i': 6, 'is': 7, 'language': 8, 'learning': 9, 'love': 10, 'natural': 11, 'of': 12, 'part': 13, 'powerful': 14, 'processing': 15, 'the': 16, 'useful': 17, 'world': 18}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습 데이터 생성 (CBOW 방식)\n",
        "window_size = 2  # 중심 단어 기준 좌우 몇 개의 단어를 고려할지\n",
        "training_data = []\n",
        "for sentence in tokenized_corpus:\n",
        "  for i, word in enumerate(sentence):\n",
        "    context_words = []\n",
        "\n",
        "    # 주변 단어 찾기 (윈도우 크기 고려)\n",
        "    for j in range(-window_size, window_size + 1):\n",
        "      if j == 0 or i + j < 0 or i + j >= len(sentence):\n",
        "        continue\n",
        "      context_words.append(word2idx[sentence[i + j]])\n",
        "\n",
        "    if len(context_words) > 0:\n",
        "      training_data.append((context_words, word2idx[word]))  # (주변 단어들, 중심 단어)\n",
        "\n",
        "print(\"\\n✅ 학습 데이터 (주변 단어 → 중심 단어):\")\n",
        "print([([idx2word[c] for c in ctx], idx2word[t]) for ctx, t in training_data])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQDDGAWnFsQN",
        "outputId": "8e2ae860-4b05-4490-ee99-ffe731f6d235"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 학습 데이터 (주변 단어 → 중심 단어):\n",
            "[(['is', 'changing'], 'ai'), (['ai', 'changing', 'the'], 'is'), (['ai', 'is', 'the', 'world'], 'changing'), (['is', 'changing', 'world'], 'the'), (['changing', 'the'], 'world'), (['love', 'learning'], 'i'), (['i', 'learning', 'ai'], 'love'), (['i', 'love', 'ai'], 'learning'), (['love', 'learning'], 'ai'), (['is', 'powerful'], 'ai'), (['ai', 'powerful', 'and'], 'is'), (['ai', 'is', 'and', 'useful'], 'powerful'), (['is', 'powerful', 'useful'], 'and'), (['powerful', 'and'], 'useful'), (['learning', 'is'], 'deep'), (['deep', 'is', 'a'], 'learning'), (['deep', 'learning', 'a', 'part'], 'is'), (['learning', 'is', 'part', 'of'], 'a'), (['is', 'a', 'of', 'ai'], 'part'), (['a', 'part', 'ai'], 'of'), (['part', 'of'], 'ai'), (['language', 'processing'], 'natural'), (['natural', 'processing', 'is'], 'language'), (['natural', 'language', 'is', 'exciting'], 'processing'), (['language', 'processing', 'exciting'], 'is'), (['processing', 'is'], 'exciting')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 신경망 학습을 위한 파라미터 설정\n",
        "embedding_dim = 10  # 단어 벡터 차원 수\n",
        "learning_rate = 0.005  # 수정된 학습률 (조금 증가)\n",
        "epochs = 1000  # 학습 반복 횟수\n",
        "\n",
        "# 6. 가중치 초기화 (Xavier 초기화 적용)\n",
        "W1 = np.random.randn(vocab_size, embedding_dim) / np.sqrt(vocab_size)\n",
        "W2 = np.random.randn(embedding_dim, vocab_size) / np.sqrt(embedding_dim)\n",
        "\n",
        "# 7. 원-핫 벡터 변환 함수\n",
        "def one_hot_encoding(word_index, vocab_size):\n",
        "  one_hot_vector = np.zeros(vocab_size)\n",
        "  one_hot_vector[word_index] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "yi26faR4FxOb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. CBOW 학습 과정\n",
        "for epoch in range(epochs):\n",
        "  random.shuffle(training_data)  # 데이터 셔플링 추가\n",
        "  loss = 0\n",
        "  for context_words, target in training_data:\n",
        "    # 평균 컨텍스트 벡터 계산\n",
        "    context_vectors = [one_hot_encoding(w, vocab_size) for w in context_words]\n",
        "    x = np.mean(context_vectors, axis=0)  # 여러 개의 원-핫 벡터 평균\n",
        "\n",
        "    # 순전파 (Forward propagation)\n",
        "    hidden_layer = np.dot(x, W1)\n",
        "    output_layer = np.dot(hidden_layer, W2)\n",
        "\n",
        "    # Softmax 오버플로우 방지\n",
        "    exp_out = np.exp(output_layer - np.max(output_layer))\n",
        "    y_pred = exp_out / np.sum(exp_out)\n",
        "\n",
        "    # 정답 벡터\n",
        "    y_true = one_hot_encoding(target, vocab_size)\n",
        "\n",
        "    # 손실 계산 (Cross-Entropy Loss, 정규화 추가)\n",
        "    loss += -np.sum(y_true * np.log(y_pred + 1e-9)) / len(training_data)\n",
        "\n",
        "    # 역전파 (Backpropagation)\n",
        "    error = y_pred - y_true\n",
        "    W2 -= learning_rate * np.outer(hidden_layer, error)\n",
        "    W1 -= learning_rate * np.outer(x, np.dot(error, W2.T))\n",
        "\n",
        "  # 100번마다 손실 출력\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {loss:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rswI65LzGixZ",
        "outputId": "1b2ffc97-70d6-40b9-f865-14e798df1ba3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000 - Avg Loss: 2.650060\n",
            "Epoch 200/1000 - Avg Loss: 2.243334\n",
            "Epoch 300/1000 - Avg Loss: 1.791765\n",
            "Epoch 400/1000 - Avg Loss: 1.405406\n",
            "Epoch 500/1000 - Avg Loss: 1.091637\n",
            "Epoch 600/1000 - Avg Loss: 0.849857\n",
            "Epoch 700/1000 - Avg Loss: 0.662646\n",
            "Epoch 800/1000 - Avg Loss: 0.520169\n",
            "Epoch 900/1000 - Avg Loss: 0.415750\n",
            "Epoch 1000/1000 - Avg Loss: 0.340371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. 학습된 단어 벡터 출력\n",
        "print(\"\\n✅ 학습된 'AI'의 벡터:\")\n",
        "print(W1[word2idx[\"ai\"]])\n",
        "\n",
        "# 10. 단어 유사도 계산 함수\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "print(\"\\n✅ 'AI'와 'learning'의 유사도:\")\n",
        "print(cosine_similarity(W1[word2idx[\"ai\"]], W1[word2idx[\"learning\"]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE5n1hJHGwUq",
        "outputId": "1c5a64ea-30aa-4bcf-87aa-e1a444c9da91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 학습된 'AI'의 벡터:\n",
            "[ 2.07911962 -0.95475846  0.38795507  1.45212952 -0.2713259   2.6087246\n",
            "  0.06722047  0.36669713 -1.67076698 -0.67336714]\n",
            "\n",
            "✅ 'AI'와 'learning'의 유사도:\n",
            "0.2374158598807644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Word2Vec: Skip-Gram"
      ],
      "metadata": {
        "id": "JbWVRHcrHhXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 1. 샘플 문장 데이터 (단순한 문장 몇 개)\n",
        "corpus = [\n",
        "  \"AI is changing the world\",\n",
        "  \"I love learning AI\",\n",
        "  \"AI is powerful and useful\",\n",
        "  \"Deep learning is a part of AI\",\n",
        "  \"Natural language processing is exciting\"\n",
        "]\n",
        "\n",
        "# 2. 데이터 전처리: 단어 토큰화\n",
        "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
        "\n",
        "# 3. 단어 사전 구축\n",
        "word_list = sorted(set(sum(tokenized_corpus, [])))  # 중복 없는 단어 리스트\n",
        "word2idx = {word: i for i, word in enumerate(word_list)}  # 단어 -> 인덱스\n",
        "idx2word = {i: word for word, i in word2idx.items()}  # 인덱스 -> 단어\n",
        "vocab_size = len(word_list)  # 단어 개수\n",
        "\n",
        "print(\"\\n✅ 단어 사전:\", word2idx)"
      ],
      "metadata": {
        "id": "aIEfTsKVHhds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e612456-10e4-4727-910b-bcc418602352"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 단어 사전: {'a': 0, 'ai': 1, 'and': 2, 'changing': 3, 'deep': 4, 'exciting': 5, 'i': 6, 'is': 7, 'language': 8, 'learning': 9, 'love': 10, 'natural': 11, 'of': 12, 'part': 13, 'powerful': 14, 'processing': 15, 'the': 16, 'useful': 17, 'world': 18}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습 데이터 생성 (Skip-gram 방식)\n",
        "window_size = 2  # 중심 단어 기준 좌우 몇 개의 단어를 고려할지\n",
        "training_data = []\n",
        "for sentence in tokenized_corpus:\n",
        "  for i, word in enumerate(sentence):\n",
        "    center_word = word2idx[word]\n",
        "    context_words = []\n",
        "\n",
        "    # 주변 단어 찾기 (윈도우 크기 고려)\n",
        "    for j in range(-window_size, window_size + 1):\n",
        "      if j == 0 or i + j < 0 or i + j >= len(sentence):\n",
        "        continue\n",
        "      context_words.append(word2idx[sentence[i + j]])\n",
        "\n",
        "    # 학습 데이터 추가\n",
        "    for context in context_words:\n",
        "      training_data.append((center_word, context))\n",
        "\n",
        "print(\"\\n✅ 학습 데이터 (중심 단어 → 주변 단어):\")\n",
        "print([(idx2word[c], idx2word[t]) for c, t in training_data])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkD1dAWcBQXw",
        "outputId": "3995124b-e1fd-43d6-80c2-2468effcdd06"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 학습 데이터 (중심 단어 → 주변 단어):\n",
            "[('ai', 'is'), ('ai', 'changing'), ('is', 'ai'), ('is', 'changing'), ('is', 'the'), ('changing', 'ai'), ('changing', 'is'), ('changing', 'the'), ('changing', 'world'), ('the', 'is'), ('the', 'changing'), ('the', 'world'), ('world', 'changing'), ('world', 'the'), ('i', 'love'), ('i', 'learning'), ('love', 'i'), ('love', 'learning'), ('love', 'ai'), ('learning', 'i'), ('learning', 'love'), ('learning', 'ai'), ('ai', 'love'), ('ai', 'learning'), ('ai', 'is'), ('ai', 'powerful'), ('is', 'ai'), ('is', 'powerful'), ('is', 'and'), ('powerful', 'ai'), ('powerful', 'is'), ('powerful', 'and'), ('powerful', 'useful'), ('and', 'is'), ('and', 'powerful'), ('and', 'useful'), ('useful', 'powerful'), ('useful', 'and'), ('deep', 'learning'), ('deep', 'is'), ('learning', 'deep'), ('learning', 'is'), ('learning', 'a'), ('is', 'deep'), ('is', 'learning'), ('is', 'a'), ('is', 'part'), ('a', 'learning'), ('a', 'is'), ('a', 'part'), ('a', 'of'), ('part', 'is'), ('part', 'a'), ('part', 'of'), ('part', 'ai'), ('of', 'a'), ('of', 'part'), ('of', 'ai'), ('ai', 'part'), ('ai', 'of'), ('natural', 'language'), ('natural', 'processing'), ('language', 'natural'), ('language', 'processing'), ('language', 'is'), ('processing', 'natural'), ('processing', 'language'), ('processing', 'is'), ('processing', 'exciting'), ('is', 'language'), ('is', 'processing'), ('is', 'exciting'), ('exciting', 'processing'), ('exciting', 'is')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 신경망 학습을 위한 파라미터 설정\n",
        "embedding_dim = 10  # 단어 벡터 차원 수\n",
        "learning_rate = 0.005  # 수정된 학습률 (조금 증가)\n",
        "epochs = 1000  # 학습 반복 횟수\n",
        "\n",
        "# 6. 가중치 초기화 (Xavier 초기화 적용)\n",
        "W1 = np.random.randn(vocab_size, embedding_dim) / np.sqrt(vocab_size)\n",
        "W2 = np.random.randn(embedding_dim, vocab_size) / np.sqrt(embedding_dim)\n",
        "\n",
        "# 7. 원-핫 벡터 변환 함수\n",
        "def one_hot_encoding(word_index, vocab_size):\n",
        "  one_hot_vector = np.zeros(vocab_size)\n",
        "  one_hot_vector[word_index] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "NhMG6zbAHhhB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Skip-gram 학습 과정\n",
        "for epoch in range(epochs):\n",
        "  random.shuffle(training_data)  # 데이터 셔플링 추가\n",
        "  loss = 0\n",
        "  for center, context in training_data:\n",
        "    # 원-핫 인코딩 (입력)\n",
        "    x = one_hot_encoding(center, vocab_size)\n",
        "\n",
        "    # 순전파 (Forward propagation)\n",
        "    hidden_layer = np.dot(x, W1)\n",
        "    output_layer = np.dot(hidden_layer, W2)\n",
        "\n",
        "    # Softmax 오버플로우 방지\n",
        "    exp_out = np.exp(output_layer - np.max(output_layer))\n",
        "    y_pred = exp_out / np.sum(exp_out)\n",
        "\n",
        "    # 정답 벡터\n",
        "    y_true = one_hot_encoding(context, vocab_size)\n",
        "\n",
        "    # 손실 계산 (Cross-Entropy Loss, 정규화 추가)\n",
        "    loss += -np.sum(y_true * np.log(y_pred + 1e-9)) / len(training_data)\n",
        "\n",
        "    # 역전파 (Backpropagation)\n",
        "    error = y_pred - y_true\n",
        "    W2 -= learning_rate * np.outer(hidden_layer, error)\n",
        "    W1 -= learning_rate * np.outer(x, np.dot(error, W2.T))\n",
        "\n",
        "  # 100번마다 손실 출력\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {loss:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZP3BnRpDKp7",
        "outputId": "28a14dc9-03c2-49a3-e6df-67466d5e03a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000 - Avg Loss: 2.373285\n",
            "Epoch 200/1000 - Avg Loss: 1.870219\n",
            "Epoch 300/1000 - Avg Loss: 1.654505\n",
            "Epoch 400/1000 - Avg Loss: 1.587934\n",
            "Epoch 500/1000 - Avg Loss: 1.563087\n",
            "Epoch 600/1000 - Avg Loss: 1.551839\n",
            "Epoch 700/1000 - Avg Loss: 1.545732\n",
            "Epoch 800/1000 - Avg Loss: 1.542455\n",
            "Epoch 900/1000 - Avg Loss: 1.540409\n",
            "Epoch 1000/1000 - Avg Loss: 1.538818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. 학습된 단어 벡터 출력\n",
        "print(\"\\n✅ 학습된 'AI'의 벡터:\")\n",
        "print(W1[word2idx[\"ai\"]])\n",
        "\n",
        "# 10. 단어 유사도 계산 함수\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "print(\"\\n✅ 'AI'와 'learning'의 유사도:\")\n",
        "print(cosine_similarity(W1[word2idx[\"ai\"]], W1[word2idx[\"learning\"]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FawJakwXDOnJ",
        "outputId": "547651e5-9be8-495e-d032-3a6640bad6ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 학습된 'AI'의 벡터:\n",
            "[-0.62533287  0.78715706  0.82410121 -0.88558919 -0.65744717  0.40566861\n",
            " -0.18185193  1.91765001  0.18919656 -1.66323656]\n",
            "\n",
            "✅ 'AI'와 'learning'의 유사도:\n",
            "-0.17754438125541705\n"
          ]
        }
      ]
    }
  ]
}